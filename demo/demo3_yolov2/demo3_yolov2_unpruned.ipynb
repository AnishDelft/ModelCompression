{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:04:22.212143Z",
     "start_time": "2019-05-25T17:04:22.140343Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      " - In Path :  /home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1\n",
      " - USE_GPU :  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "dir_main = os.path.abspath('../../../repo1/')\n",
    "sys.path.append(dir_main)\n",
    "print (' - In Path : ', sys.path[-1])\n",
    "\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print (' - USE_GPU : ', USE_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model - Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T20:06:57.984149Z",
     "start_time": "2019-05-22T20:06:57.770713Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from src.nets import *\n",
    "\n",
    "if (1):\n",
    "    cfgfile    = os.path.join(dir_main, 'data/cfg/github_pjreddie/yolov2-voc.cfg')\n",
    "    weightfile = os.path.join(dir_main, 'data/weights/github_pjreddie/yolov2-voc.weights')\n",
    "    # wget https://pjreddie.com/media/files/yolov2-voc.weights\n",
    "    # model = getYOLOv2(cfgfile, weightfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model - Eval (mAP=0.7513)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T20:46:17.644745Z",
     "start_time": "2019-05-22T20:46:12.980052Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Reading predictions from :  /home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/demo/demo3_yolov2/eval_data ( comp4_det_test_ )\n",
      " - VOC07 metric? Yes\n",
      "\n",
      "~~~~~~~~\n",
      "          class       mAP\n",
      "0     aeroplane  0.776270\n",
      "1       bicycle  0.798833\n",
      "2          bird  0.753812\n",
      "3          boat  0.653453\n",
      "4        bottle  0.494928\n",
      "5           bus  0.816337\n",
      "6           car  0.829606\n",
      "7           cat  0.882470\n",
      "8         chair  0.568349\n",
      "9           cow  0.804215\n",
      "10  diningtable  0.721661\n",
      "11          dog  0.854689\n",
      "12        horse  0.860699\n",
      "13    motorbike  0.806348\n",
      "14       person  0.759239\n",
      "15  pottedplant  0.521379\n",
      "16        sheep  0.773517\n",
      "17         sofa  0.733054\n",
      "18        train  0.858911\n",
      "19    tvmonitor  0.757254\n",
      "~~~~~~~~\n",
      "Mean AP = 0.7513\n"
     ]
    }
   ],
   "source": [
    "from src.predict import *\n",
    "\n",
    "DIR_PROJ         = '/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1'\n",
    "MODEL_CFGFILE    = os.path.join(DIR_PROJ, 'data/cfg/github_pjreddie/yolov2-voc.cfg')\n",
    "MODEL_WEIGHTFILE = os.path.join(DIR_PROJ, 'data/weights/github_pjreddie/yolov2-voc.weights')\n",
    "PASCAL_DIR       = os.path.join(DIR_PROJ, 'data/dataset/VOCdevkit/')\n",
    "EVAL_IMAGELIST   = os.path.join(DIR_PROJ, 'data/dataset/2007_test.txt')\n",
    "EVAL_OUTPUTDIR   = os.path.join(DIR_PROJ, 'demo/demo3_yolov2/eval_data')\n",
    "EVAL_PREFIX      = 'comp4_det_test_'#'pretrained_yolov2_'\n",
    "EVAL_OUTPUTDIR_PKL = os.path.join(DIR_PROJ, 'demo/demo3_yolov2/eval_results')\n",
    "\n",
    "valObj = PASCALVOCEval(MODEL_CFGFILE, MODEL_WEIGHTFILE, PASCAL_DIR, EVAL_IMAGELIST, EVAL_OUTPUTDIR, EVAL_PREFIX, EVAL_OUTPUTDIR_PKL)\n",
    "# valObj.predict()\n",
    "valObj._do_python_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Eval -2 (90%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T15:55:45.055172Z",
     "start_time": "2019-05-25T15:42:46.179445Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1. Loading model\n",
      " - 2. Loading dataset\n",
      " - Validating : Images :  2476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/src/nets2_utils.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_confs = torch.nn.Softmax()(Variable(output[5:5+num_classes].transpose(0,1))).data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Reading predictions from :  /home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/demo/demo3_yolov2/eval_data ( iter1_90_ )\n",
      " - VOC07 metric? Yes\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-baa10f3979c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvalObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPASCALVOCEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CFGFILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_WEIGHTFILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPASCAL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVAL_IMAGELIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVAL_OUTPUTDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVAL_PREFIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVAL_OUTPUTDIR_PKL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mvalObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# valObj._do_python_eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/src/predict.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, CONF_THRESH, NMS_THRESH)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mfps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_python_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/src/predict.py\u001b[0m in \u001b[0;36m_do_python_eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 rec, prec, ap = self.voc_eval(\n\u001b[1;32m    333\u001b[0m                     \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannopath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimagesetfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcachedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0movthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                     use_07_metric=use_07_metric)\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0maps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mfinalMAP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/src/predict.py\u001b[0m in \u001b[0;36mvoc_eval\u001b[0;34m(self, detpath, annopath, imagesetfile, classname, cachedir, ovthresh, use_07_metric)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0msorted_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0msorted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mBB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msorted_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0mimage_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "from src.predict import *\n",
    "\n",
    "DIR_PROJ         = '/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1'\n",
    "MODEL_CFGFILE    = os.path.join(DIR_PROJ, 'data/cfg/github_pjreddie/yolov2-voc.cfg')\n",
    "MODEL_WEIGHTFILE = os.path.join(DIR_PROJ, 'data/weights/pruned/yolov2-voc-weight-prune-90.weights')\n",
    "PASCAL_DIR       = os.path.join(DIR_PROJ, 'data/dataset/VOCdevkit/')\n",
    "EVAL_IMAGELIST   = os.path.join(DIR_PROJ, 'data/dataset/2007_test.txt')\n",
    "EVAL_OUTPUTDIR   = os.path.join(DIR_PROJ, 'demo/demo3_yolov2/eval_data')\n",
    "EVAL_PREFIX      = 'iter1_90_'\n",
    "EVAL_OUTPUTDIR_PKL = os.path.join(DIR_PROJ, 'demo/demo3_yolov2/eval_results')\n",
    "\n",
    "valObj = PASCALVOCEval(MODEL_CFGFILE, MODEL_WEIGHTFILE, PASCAL_DIR, EVAL_IMAGELIST, EVAL_OUTPUTDIR, EVAL_PREFIX, EVAL_OUTPUTDIR_PKL)\n",
    "valObj.predict()\n",
    "# valObj._do_python_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model - Eval -3 (10%) (mAP=0.7586)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T16:27:06.352227Z",
     "start_time": "2019-05-25T16:05:32.870264Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1. Loading model\n",
      " - 2. Loading dataset\n",
      " - Validating : Images :  2476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/src/nets2_utils.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_confs = torch.nn.Softmax()(Variable(output[5:5+num_classes].transpose(0,1))).data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Reading predictions from :  /home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/demo/demo3_yolov2/eval_data ( iter1_90_ )\n",
      " - VOC07 metric? Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~~~~~~\n",
      "          class       mAP\n",
      "0     aeroplane  0.782459\n",
      "1       bicycle  0.819837\n",
      "2          bird  0.761448\n",
      "3          boat  0.642047\n",
      "4        bottle  0.506293\n",
      "5           bus  0.824128\n",
      "6           car  0.834679\n",
      "7           cat  0.881470\n",
      "8         chair  0.581790\n",
      "9           cow  0.810257\n",
      "10  diningtable  0.745099\n",
      "11          dog  0.854841\n",
      "12        horse  0.862790\n",
      "13    motorbike  0.825609\n",
      "14       person  0.777588\n",
      "15  pottedplant  0.542147\n",
      "16        sheep  0.761144\n",
      "17         sofa  0.750487\n",
      "18        train  0.859133\n",
      "19    tvmonitor  0.748598\n",
      "~~~~~~~~\n",
      "Mean AP = 0.7586\n"
     ]
    }
   ],
   "source": [
    "from src.predict import *\n",
    "\n",
    "DIR_PROJ         = '/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1'\n",
    "MODEL_CFGFILE    = os.path.join(DIR_PROJ, 'data/cfg/github_pjreddie/yolov2-voc.cfg')\n",
    "MODEL_WEIGHTFILE = os.path.join(DIR_PROJ, 'data/weights/pruned/yolov2-voc-weight-prune-10.0.weights')\n",
    "PASCAL_DIR       = os.path.join(DIR_PROJ, 'data/dataset/VOCdevkit/')\n",
    "EVAL_IMAGELIST   = os.path.join(DIR_PROJ, 'data/dataset/2007_test.txt')\n",
    "EVAL_OUTPUTDIR   = os.path.join(DIR_PROJ, 'demo/demo3_yolov2/eval_data')\n",
    "EVAL_PREFIX      = 'iter1_90_'\n",
    "EVAL_OUTPUTDIR_PKL = os.path.join(DIR_PROJ, 'demo/demo3_yolov2/eval_results')\n",
    "\n",
    "valObj = PASCALVOCEval(MODEL_CFGFILE, MODEL_WEIGHTFILE, PASCAL_DIR, EVAL_IMAGELIST, EVAL_OUTPUTDIR, EVAL_PREFIX, EVAL_OUTPUTDIR_PKL)\n",
    "valObj.predict()\n",
    "# valObj._do_python_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Model - Eval -4 (50%) (mAP=0.7597)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T16:55:06.761608Z",
     "start_time": "2019-05-25T16:34:32.521884Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1. Loading model\n",
      " - 2. Loading dataset\n",
      " - Validating : Images :  2476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/src/nets2_utils.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_confs = torch.nn.Softmax()(Variable(output[5:5+num_classes].transpose(0,1))).data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Reading predictions from :  /home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/demo/demo3_yolov2/eval_data ( iter1_weighspruned_50_ )\n",
      " - VOC07 metric? Yes\n",
      "\n",
      "~~~~~~~~\n",
      "          class       mAP\n",
      "0     aeroplane  0.789377\n",
      "1       bicycle  0.820896\n",
      "2          bird  0.767867\n",
      "3          boat  0.634567\n",
      "4        bottle  0.495231\n",
      "5           bus  0.840878\n",
      "6           car  0.846564\n",
      "7           cat  0.876844\n",
      "8         chair  0.590701\n",
      "9           cow  0.817039\n",
      "10  diningtable  0.747886\n",
      "11          dog  0.853449\n",
      "12        horse  0.864122\n",
      "13    motorbike  0.820361\n",
      "14       person  0.777582\n",
      "15  pottedplant  0.532764\n",
      "16        sheep  0.752805\n",
      "17         sofa  0.763504\n",
      "18        train  0.863236\n",
      "19    tvmonitor  0.738208\n",
      "~~~~~~~~\n",
      "Mean AP = 0.7597\n"
     ]
    }
   ],
   "source": [
    "from src.predict import *\n",
    "\n",
    "DIR_PROJ         = '/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1'\n",
    "MODEL_CFGFILE    = os.path.join(DIR_PROJ, 'data/cfg/github_pjreddie/yolov2-voc.cfg')\n",
    "MODEL_WEIGHTFILE = os.path.join(DIR_PROJ, 'data/weights/pruned/yolov2-voc-weight-prune-50.0.weights')\n",
    "PASCAL_DIR       = os.path.join(DIR_PROJ, 'data/dataset/VOCdevkit/')\n",
    "EVAL_IMAGELIST   = os.path.join(DIR_PROJ, 'data/dataset/2007_test.txt')\n",
    "EVAL_OUTPUTDIR   = os.path.join(DIR_PROJ, 'demo/demo3_yolov2/eval_data')\n",
    "EVAL_PREFIX      = 'iter1_50_'\n",
    "EVAL_OUTPUTDIR_PKL = os.path.join(DIR_PROJ, 'demo/demo3_yolov2/eval_results')\n",
    "\n",
    "valObj = PASCALVOCEval(MODEL_CFGFILE, MODEL_WEIGHTFILE, PASCAL_DIR, EVAL_IMAGELIST, EVAL_OUTPUTDIR, EVAL_PREFIX, EVAL_OUTPUTDIR_PKL)\n",
    "valObj.predict()\n",
    "# valObj._do_python_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model - Eval -5 (70%) (mAP=0.6937)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:20:55.297463Z",
     "start_time": "2019-05-25T17:04:27.808628Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1. Loading model\n",
      " - 2. Loading dataset\n",
      " - Validating : Images :  2476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/src/nets2_utils.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_confs = torch.nn.Softmax()(Variable(output[5:5+num_classes].transpose(0,1))).data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Reading predictions from :  /home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/demo/demo3_yolov2/eval_data ( iter1_weighspruned_70_ )\n",
      " - VOC07 metric? Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~~~~~~\n",
      "          class       mAP\n",
      "0     aeroplane  0.745336\n",
      "1       bicycle  0.734107\n",
      "2          bird  0.703257\n",
      "3          boat  0.575996\n",
      "4        bottle  0.446326\n",
      "5           bus  0.784627\n",
      "6           car  0.822348\n",
      "7           cat  0.803765\n",
      "8         chair  0.528557\n",
      "9           cow  0.744448\n",
      "10  diningtable  0.628148\n",
      "11          dog  0.771520\n",
      "12        horse  0.800096\n",
      "13    motorbike  0.749041\n",
      "14       person  0.736751\n",
      "15  pottedplant  0.410977\n",
      "16        sheep  0.684337\n",
      "17         sofa  0.710636\n",
      "18        train  0.780860\n",
      "19    tvmonitor  0.712777\n",
      "~~~~~~~~\n",
      "Mean AP = 0.6937\n"
     ]
    }
   ],
   "source": [
    "from src.predict import *\n",
    "\n",
    "DIR_PROJ         = '/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1'\n",
    "MODEL_CFGFILE    = os.path.join(DIR_PROJ, 'data/cfg/github_pjreddie/yolov2-voc.cfg')\n",
    "MODEL_WEIGHTFILE = os.path.join(DIR_PROJ, 'data/weights/pruned/yolov2-voc-weight-prune-70.0.weights')\n",
    "PASCAL_DIR       = os.path.join(DIR_PROJ, 'data/dataset/VOCdevkit/')\n",
    "EVAL_IMAGELIST   = os.path.join(DIR_PROJ, 'data/dataset/2007_test.txt')\n",
    "EVAL_OUTPUTDIR   = os.path.join(DIR_PROJ, 'demo/demo3_yolov2/eval_data')\n",
    "EVAL_PREFIX      = 'iter1_weighspruned_70_'\n",
    "EVAL_OUTPUTDIR_PKL = os.path.join(DIR_PROJ, 'demo/demo3_yolov2/eval_results')\n",
    "\n",
    "valObj = PASCALVOCEval(MODEL_CFGFILE, MODEL_WEIGHTFILE, PASCAL_DIR, EVAL_IMAGELIST, EVAL_OUTPUTDIR, EVAL_PREFIX, EVAL_OUTPUTDIR_PKL)\n",
    "valObj.predict()\n",
    "# valObj._do_python_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:34:39.452269Z",
     "start_time": "2019-05-25T17:34:22.006649Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - year :  2012  || image_set :  train\n",
      " - year :  2012  || image_set :  val\n",
      " - year :  2007  || image_set :  train\n",
      " - year :  2007  || image_set :  val\n",
      " - year :  2007  || image_set :  test\n"
     ]
    }
   ],
   "source": [
    "from src.dataloader import *\n",
    "DATA_DIR = '/home/strider/Work/Netherlands/TUDelft/1_Courses/Sem2/DeepLearning/Project/repo1/data/dataset'\n",
    "setup_VOC(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {
    "1b83bc57e4614034bacdc1542b6f3b33": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "300412370b6a4eeeaa368afa2c5b1959": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "5fbb7c81af004a5a92f5842199ddcab3": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "7c1290cae82a4c4598cb3eb2a88b76f0": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "9578b97e283a44b09e18bffdbad72853": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "a4092682a53b4ef7aa9aaee747c442e6": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "c70a39bf58b844e4b9671fa6d618c52f": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "fcb68737796b4a00851f2226d19cf0b3": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
